![image](https://github.com/KajaMarinsek/2024_AppliedDataScience/blob/main/Data%20Modeling%20Project%20-%20Kickstarter/images/OrangeTinyIcon.png)

 * To gain insight into Kickstarter data that I found in the [Orange](https://orangedatamining.com/) 
and where I ran the same test split to see the model performance scores:


![image](https://github.com/KajaMarinsek/2024_AppliedDataScience/blob/main/Data%20Modeling%20Project%20-%20Kickstarter/images/AverageScoresKickstarter.png)

![image](https://github.com/KajaMarinsek/2024_AppliedDataScience/blob/main/Data%20Modeling%20Project%20-%20Kickstarter/images/OrangeTinyIcon.png)
Yes Scores ![image](https://github.com/KajaMarinsek/2024_AppliedDataScience/blob/main/Data%20Modeling%20Project%20-%20Kickstarter/images/YESscoresKickstarter.png)

![image](https://github.com/KajaMarinsek/2024_AppliedDataScience/blob/main/Data%20Modeling%20Project%20-%20Kickstarter/images/OrangeTinyIcon.png)
No Scores  ![image](https://github.com/KajaMarinsek/2024_AppliedDataScience/blob/main/Data%20Modeling%20Project%20-%20Kickstarter/images/NOscoresKickstarter.png)



### After I ran my two .ipynb files to gain Scikit-learn and Pycaret results of classification of same Kickstarter data:

 __Accuracy:__ It measures the overall correctness of the model across all classes.
In my case, the accuracy is approximately 81.12%. This means that roughly 81.12% of the predictions made by the model were correct.

__Precision:__ Precision is a measure of the correctness achieved in positive predictions made by the model. It's calculated as the ratio of true positives to the sum of true positives and false positives. In my case, the precision is approximately 75.24%. This means that when the model predicted a positive outcome, it was correct about 75.24% of the time.

__Recall:__ Recall, also known as sensitivity, measures the completeness of the model. It calculates the ratio of true positives to the sum of true positives and false negatives. In my case, the recall is approximately 81.44%. This means that out of all the actual positive instances, the model correctly identified about 81.44% of them.
